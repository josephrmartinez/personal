---
title: "CloudAtlas"
description: "Fine tune a machine learning model to identify cloud types."
pubDate: "June 11, 2023"
---

CloudAtlas is a web app that uses a fine-tuned machine learning object detection model to help users identify cloud types.

A user uploads a photo (either from their mobile phone camera or from an image stored on their device) and then when they click "submit", the image is sent to a computer vision model that I have fine-tuned solely for the purpose of recognizing cloud types. The model takes a few seconds to analyze the image and then returns a percentage that indicates the confidence that the image is of a certain cloud type.

The core of this app is an image classification neural network that I've fine tuned to distinguish between ten different cloud types. Fine tuning is a transfer learning technique where the parameters of a pretrained model are updated by additional rounds (epochs) of training on a new set of data for our specific use case. We start with a pre-trained general purpose model that can already recognize basic shapes and objects. Then, in this case, the model is fine-tuned until it achieves an acceptable ability to identify a "cumulonimbus cloud" from an image uploaded by a user.

### Steps to Train and Deploy the Model

Training an AI to recognize cloud types involves a process known as supervised learning, where you provide the AI with a labeled dataset of cloud images along with their corresponding cloud types. These are the general steps to train an AI for cloud type recognition:

**Data collection:** Gather a large dataset of cloud images with accurate labels indicating the corresponding cloud types. In this case, I collected images from a DuckDuckGo search.

**Data preprocessing:** Clean and preprocess the dataset by resizing the images to a consistent size, normalizing the pixel values, and splitting the data into training and testing sets. It's important to have a balanced representation of each cloud type in your dataset to avoid bias during training.

**Model selection:** Choose an appropriate machine learning model for image recognition tasks. Convolutional Neural Networks (CNNs) are commonly used for image classification tasks due to their ability to learn spatial patterns. In this case, I started with a pre-trained ResNet model, which has already been trained on large-scale image datasets.

**Model training:** Train the ResNet model using the labeled cloud image dataset. During training, the model learns to extract features from the images and map them to the corresponding cloud types. This process involves feeding the training data through the network, calculating the loss, and updating the model's parameters through backpropagation and optimization algorithms (e.g., gradient descent).

**Model evaluation:** Evaluate the trained model using the testing dataset to assess its performance. Metrics such as accuracy, precision, recall, and F1 score can be used to measure the model's effectiveness in recognizing different cloud types.

**Model refinement:** If the model's performance is not satisfactory, you can refine it by adjusting hyperparameters, modifying the network architecture, or increasing the size and diversity of the training dataset. Iterative refinement is often necessary to improve the model's accuracy.

**Deployment and inference:** Once you have a trained model with satisfactory performance, you can deploy it to make predictions on new, unseen cloud images. The AI model takes an input image and outputs the predicted cloud type based on its learned knowledge.

### Developing the model on Colab with fast.ai

The fast.ai library makes all of these steps surprisingly easy. I can take care of everything from gathering the data up to deployment within a juptyper notebook running inside of Colab or Kaggle:

```
pip install -Uqq fastbook
import fastbook
fastbook.setup_book()
from fastbook import *
from fastai.vision.widgets import *
```

**Data Collection: Gather and Label Data**

Start by creating a list of cloud types we are going to identify. Make a folder for each of these cloud types. Run an image search on Duck Duck Go for 100 each of these cloud types and put the images in their respective folder. Since we are gathering images from an online search, there are likely to be corrupt files. Identify these failed images with verify_images() and then remove these files by running unLink on each of them:

```
cloud_types = 'cirrus', 'cirrocumulus', 'cirrostratus', 'altocumulus',
'altostratus', 'nimbostratus', 'stratocumulus', 'stratus', 'cumulus',
'cumulonimbus'

path = Path('clouds')

if not path.exists():
    path.mkdir()
    for o in cloud_types:
        dest = (path/o)
        dest.mkdir(exist_ok=True)
        results = search_images_ddg(f'{o} clouds', 100)
        download_images(dest, urls=results)

fns = get_image_files(path)

failed = verify_images(fns)
failed.map(Path.unlink);
```

**Data Preprocessing: Organize Data for Model Training**

Now that we have the data downloaded and sorted into folders, we need to organize it in a format suitable for model training. The fast.ai library includes a DataBlock object and dataloaders function that makes this incredibly straightforward:

```
clouds = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=parent_label,
    item_tfms=Resize(128))

dls = clouds.dataloaders(path)
```

In the block argument of the DataBlock object we provide a tuple where we set the cloud images as the independent variable and the cloud type labels as the dependent variable. The RandomSplitter function sets aside twenty percent of the images for a validation set. parent_label is a fastai function that simply returns thee name of the folder that a file is in. Since we put each of our cloud images into a folder with the name of the cloud type, this provides the labels for our items. Finally, we apply a transformation on each item with the Resize function so that each of the images are the exact same size.

The DataBlock is like a template for creating a dataloader object. We create a dataloaders object (dls) by then passing the actual path to our data (and labels).

**Model Selection and Training**

For this project, we are going to use the ResNet18 Convolutional Neural Network. Using the fastai library, we can simply specify resnet18 as as argument on a vision_learner function.

We first create a vision_learner object with the dataloader (dls) and model (resnet18). Then call the fine_tune function on the vision_learner object to specify the number of epochs (training cycles) we want to run on the model. More is not always better. At a certain point, you may run into overfitting your model. In this case, I just ran four epochs:

```
learn = vision_learner(dls, resnet18, metrics=error_rate)
learn.fine_tune(4)
```

**Model Evaluation and Refinement: Confusion Matrix and ImageClassifierCleaner**

fastai includes a confusion maxtrix module that provides a visual way to evaluate how the model is performing. Simply running these two lines of code pops out a graphic representation of how well the model did with the predictions:

```
interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix()
```

I ran the model, got this output:

/img/

fastai also includes a handy graphic user interface tool called an ImageClassifierCleaner that allows me to pull out or recategorizee problematic data:

cleaner = ImageClassifierCleaner(learn)
cleaner

I clean the data and then retrain the model.
This is my output:

/img2/

You will notice that this one section improved, but another actually did worse. Now I can focus on that area and see if there are mistakes I actually introduced during the data cleaning. Fastai makes it very simple to just keep repeating this process until I get to an acceptable result. Since we're running a cloud detection model and not a cancer detection model, a degree of inaccuracy is acceptable. Additionally, it is likely that some images include multiple types of clouds in the same frame. So in this case it is actually totally fine for the output to be a cloud type prediction that is split between multiple types -- there may in fact be multiple types of clouds in the frame!

**Deployment and Inference**
Once I am satisfied with the accuracy of my model, I can export it simply by running:

`learn.export()`

In my case, I end up with a 45mb [.pkl](https://docs.python.org/3/library/pickle.html) file. That's it. In order to do the model training / fine-tuning, it is best to have access to a GPU. But then to use this model in production, I can simply run it off a consumer grade CPU or upload the model online. HuggingFace is an online community that provides a very simple platform for sharing these models with a straightforward user interface.

For the time-being, I'm just deploying the model on HuggingFace with the standard Gradio UI. This allows users to quickly upload and crop photos and then click 'submit' to run the model. Results are displayed within seconds below.

The entire app.py file is tiny:

```
import gradio as gr
from fastai.vision.all import *

learn = load_learner('export.pkl')
categories = ('altocumulus', 'altostratus', 'cirrocumulus', 'cirrostratus',
'cirrus', 'cumulonimbus', 'cumulus', 'nimbostratus', 'stratocumulus', 'stratus')

def classifier(img):
    pred, idx, prob = learn.predict(img)
    return dict(zip(categories, map(float, prob)))

image = gr.inputs.Image(shape = (192, 192))
label = gr.outputs.Label()

iface = gr.Interface(fn=classifier, inputs=image, outputs=label)
iface.launch(inline = False)
```

### Areas for improvement

Going forward, I could fine tune the model further and create a UI that offers functionality beyond the model.
